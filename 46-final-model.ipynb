{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 46-final-model\n",
    "\n",
    "> Compare 3 different models\n",
    "\n",
    "> Train final model on 2015-2018-Aug data, test on 2018-Sept data, predict 2018-Nov by 2018-Oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_process(file, real_x, real_y, model, test_size=0.25, seed=114514):\n",
    "    new_data = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/\"+file)\n",
    "    new_data = new_data.fillna(-1)\n",
    "    X = new_data.drop('response',axis=1)\n",
    "    Y = new_data['response']\n",
    "    X_train_old, X_test_old, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)\n",
    "    train = pd.concat([X_train_old, Y_train], axis=1)\n",
    "    length = 2*len(train[train['response']==1])\n",
    "    buy = train[train['response']==1].sample(n=length, replace=True, random_state = seed)\n",
    "    nobuy = train[train['response']==0].sample(n=length, replace=True, random_state = seed)\n",
    "    new_train = pd.concat([buy,nobuy])\n",
    "    X_train = new_train.drop(['indiv_id','response','store'], axis=1)\n",
    "    Y_train = new_train['response']\n",
    "    X_test = X_test_old.drop(['indiv_id','store'], axis = 1)\n",
    "    print(\"data loaded\")\n",
    "    model = model\n",
    "    print(\"model set\")\n",
    "    eval_set = [(X_test, Y_test)]\n",
    "    model.fit(X_train, Y_train)\n",
    "    print(\"model fit\")\n",
    "    Y_prob = pd.DataFrame(model.predict_proba(X_test), columns = ['pred_0', 'pred_1'])\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(\"model predict\")\n",
    "    Real_X = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/\"+real_x)\n",
    "    Real_X = Real_X.fillna(-1)\n",
    "    Real_Y_prob = pd.DataFrame(model.predict_proba(Real_X.drop(['indiv_id', 'response','store'], axis=1)), columns = ['pred_0', 'pred_1'])\n",
    "    top100k_id_new = pd.concat([Real_X, Real_Y_prob], axis=1).sort_values('pred_1', ascending=False)['indiv_id'].unique()[0:100000]\n",
    "    Real_Y = pd.read_csv(\"/data/p_dsi/teams2022/team_1/new_data/\"+real_y)\n",
    "    Real_Y_id = Real_Y[Real_Y['prod_group_code']==5]['indiv_id'].unique()\n",
    "    index = len([x for x in top100k_id_new if x in Real_Y_id])/100000\n",
    "    print(\"index:\", index)\n",
    "    return model, top100k_id_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.07308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(),\n",
       " array([  5.32334493e+08,   3.14545170e+08,   3.22494400e+08, ...,\n",
       "          2.62157041e+08,   2.88510415e+08,   2.62156573e+08]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_17.csv','2015_18_02.csv','20180331.csv', RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.07415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(),\n",
       " array([  3.18411810e+08,   3.21146366e+08,   4.25423680e+08, ...,\n",
       "          3.14550182e+08,   2.76752632e+08,   2.72471729e+08]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_02.csv','2015_18_04.csv','20180531.csv', RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.07898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(),\n",
       " array([  2.53533886e+08,   5.04321878e+08,   2.68629500e+08, ...,\n",
       "          2.76274493e+08,   2.79193656e+08,   2.91443136e+08]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_04.csv','2015_18_06.csv','20180731.csv', RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.0754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(),\n",
       " array([  5.32631095e+08,   3.06307678e+08,   2.82381241e+08, ...,\n",
       "          2.64036656e+08,   2.88585451e+08,   3.66168699e+08]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_06.csv','2015_18_08.csv','20180930.csv', RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.11445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(),\n",
       " array([  2.73830959e+08,   2.91967266e+08,   5.42354822e+08, ...,\n",
       "          3.10656646e+08,   2.67277997e+08,   2.67928362e+08]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_17.csv','2015_18_02.csv','20180331.csv', MLPClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.11108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(),\n",
       " array([  2.61233660e+08,   4.24093950e+08,   2.56980288e+08, ...,\n",
       "          2.78547689e+08,   2.95722273e+08,   2.89846689e+08]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_02.csv','2015_18_04.csv','20180531.csv', MLPClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.11188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(),\n",
       " array([  2.81381331e+08,   4.24093950e+08,   2.81842221e+08, ...,\n",
       "          2.82784478e+08,   2.54052915e+08,   2.54617456e+08]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_04.csv','2015_18_06.csv','20180731.csv', MLPClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.11039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(),\n",
       " array([  2.56980288e+08,   2.59698763e+08,   5.32456325e+08, ...,\n",
       "          2.62964958e+08,   2.58604744e+08,   2.81410353e+08]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_06.csv','2015_18_08.csv','20180930.csv', MLPClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lins19/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "model fit\n",
      "model predict\n",
      "index: 0.10365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "               gamma=0, gpu_id=-1, importance_type=None,\n",
       "               interaction_constraints='', learning_rate=0.300000012,\n",
       "               max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "               num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None),\n",
       " array([  2.54315387e+08,   2.80719092e+08,   2.53795219e+08, ...,\n",
       "          2.67867592e+08,   5.34305983e+08,   2.78207274e+08]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_17.csv','2015_18_02.csv','20180331.csv', XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lins19/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:11:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "model fit\n",
      "model predict\n",
      "index: 0.10382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "               gamma=0, gpu_id=-1, importance_type=None,\n",
       "               interaction_constraints='', learning_rate=0.300000012,\n",
       "               max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "               num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None),\n",
       " array([  4.45870572e+08,   2.75926385e+08,   2.53849821e+08, ...,\n",
       "          2.59109931e+08,   2.54113772e+08,   2.85595293e+08]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_02.csv','2015_18_04.csv','20180531.csv', XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lins19/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:13:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "model fit\n",
      "model predict\n",
      "index: 0.1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "               gamma=0, gpu_id=-1, importance_type=None,\n",
       "               interaction_constraints='', learning_rate=0.300000012,\n",
       "               max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "               num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None),\n",
       " array([  2.63360742e+08,   2.61233660e+08,   2.89588974e+08, ...,\n",
       "          2.58404726e+08,   2.86031740e+08,   2.77227202e+08]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_04.csv','2015_18_06.csv','20180731.csv', XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lins19/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:15:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "model fit\n",
      "model predict\n",
      "index: 0.103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "               gamma=0, gpu_id=-1, importance_type=None,\n",
       "               interaction_constraints='', learning_rate=0.300000012,\n",
       "               max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "               num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None),\n",
       " array([  6.11852313e+08,   2.77364529e+08,   2.93169414e+08, ...,\n",
       "          2.80528365e+08,   2.59828104e+08,   2.66650740e+08]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process('2015_18_06.csv','2015_18_08.csv','20180930.csv', XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/2015_18_06.csv\")\n",
    "new_data = new_data.fillna(-1)\n",
    "X = new_data.drop('response',axis=1)\n",
    "Y = new_data['response']\n",
    "X_train_old, X_test_old, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 114514)\n",
    "train = pd.concat([X_train_old, Y_train], axis=1)\n",
    "length = 2*len(train[train['response']==1])\n",
    "buy = train[train['response']==1].sample(n=length, replace=True, random_state = 114514)\n",
    "nobuy = train[train['response']==0].sample(n=length, replace=True, random_state = 114514)\n",
    "new_train = pd.concat([buy,nobuy])\n",
    "X_train = new_train.drop(['indiv_id','response','store'], axis=1)\n",
    "Y_train = new_train['response']\n",
    "X_test = X_test_old.drop(['indiv_id','store'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(10, 30, 10), (20,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, Y_train) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (10, 30, 10),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict\n",
      "index: 0.11051\n"
     ]
    }
   ],
   "source": [
    "Y_prob = pd.DataFrame(clf.predict_proba(X_test), columns = ['pred_0', 'pred_1'])\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"model predict\")\n",
    "Real_X = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/2015_18_08.csv\")\n",
    "Real_X = Real_X.fillna(-1)\n",
    "Real_Y_prob = pd.DataFrame(clf.predict_proba(Real_X.drop(['indiv_id', 'response','store'], axis=1)), columns = ['pred_0', 'pred_1'])\n",
    "top100k_id_new = pd.concat([Real_X, Real_Y_prob], axis=1).sort_values('pred_1', ascending=False)['indiv_id'].unique()[0:100000]\n",
    "Real_Y = pd.read_csv(\"/data/p_dsi/teams2022/team_1/new_data/20180930.csv\")\n",
    "Real_Y_id = Real_Y[Real_Y['prod_group_code']==5]['indiv_id'].unique()\n",
    "index = len([x for x in top100k_id_new if x in Real_Y_id])/100000\n",
    "print(\"index:\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=114514\n",
    "test_size=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['indiv_id', 'total_transaction', 'sales_total', 'tire_purchases',\n",
       "       'service_purchases', 'other_purchases', 'days_since_first_transaction',\n",
       "       'days_since_last_transaction', 'days_since_first_tire_purchase',\n",
       "       'days_since_last_tire_purchase', 'vehicle_count', 'model_year_avg',\n",
       "       'region', 'tire_purchase_freq', 'response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model set\n",
      "model fit\n",
      "model predict\n",
      "index: 0.10855\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/2015_18_08.csv\")\n",
    "new_data = new_data.fillna(-1)\n",
    "X = new_data.drop('response',axis=1)\n",
    "Y = new_data['response']\n",
    "X_train_old, X_test_old, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)\n",
    "train = pd.concat([X_train_old, Y_train], axis=1)\n",
    "length = 2*len(train[train['response']==1])\n",
    "buy = train[train['response']==1].sample(n=length, replace=True, random_state = seed)\n",
    "nobuy = train[train['response']==0].sample(n=length, replace=True, random_state = seed)\n",
    "new_train = pd.concat([buy,nobuy])\n",
    "X_train = new_train.drop(['indiv_id','response'], axis=1)\n",
    "Y_train = new_train['response']\n",
    "X_test = X_test_old.drop(['indiv_id'], axis = 1)\n",
    "print(\"data loaded\")\n",
    "model = MLPClassifier(activation= 'relu', alpha=0.0001, hidden_layer_sizes= (10, 30, 10), learning_rate='adaptive',solver='adam')\n",
    "print(\"model set\")\n",
    "eval_set = [(X_test, Y_test)]\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"model fit\")\n",
    "Y_prob = pd.DataFrame(model.predict_proba(X_test), columns = ['pred_0', 'pred_1'])\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"model predict\")\n",
    "Real_X = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/2015_18_09.csv\")\n",
    "Real_X = Real_X.fillna(-1)\n",
    "Real_Y_prob = pd.DataFrame(model.predict_proba(Real_X.drop(['indiv_id', 'response'], axis=1)), columns = ['pred_0', 'pred_1'])\n",
    "top100k_id_new = pd.concat([Real_X, Real_Y_prob], axis=1).sort_values('pred_1', ascending=False)['indiv_id'].unique()[0:100000]\n",
    "Real_Y = pd.read_csv(\"/data/p_dsi/teams2022/team_1/new_data/20181031.csv\")\n",
    "Real_Y_id = Real_Y[Real_Y['prod_group_code']==5]['indiv_id'].unique()\n",
    "index = len([x for x in top100k_id_new if x in Real_Y_id])/100000\n",
    "print(\"index:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_X = pd.read_csv(\"/data/p_dsi/teams2022/team_1/fe_data/2015_18_10.csv\")\n",
    "Real_X = Real_X.fillna(-1)\n",
    "Real_Y_prob = pd.DataFrame(model.predict_proba(Real_X.drop(['indiv_id'], axis=1)), columns = ['pred_0', 'pred_1'])\n",
    "top100k_id_new = pd.concat([Real_X, Real_Y_prob], axis=1).sort_values('pred_1', ascending=False)['indiv_id'].unique()[0:100000]\n",
    "pd.DataFrame(top100k_id_new).to_csv('/data/p_dsi/teams2022/team_1/top100000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.61233660e+08,   5.32456325e+08,   2.56980288e+08, ...,\n",
       "         2.67684336e+08,   2.66689889e+08,   2.86305973e+08])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top100k_id_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
