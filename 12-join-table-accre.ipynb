{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_join_table_accre\n",
    "> Loading the data on accre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import janitor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# must install janitor package with the following shell command:\n",
    "# 'pip install --user pyjanitor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may be helpful to use following command to install janitor on ACCRE:\n",
    "#!conda install -c conda-forge/label/gcc7 pyjanitor -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/p_dsi/teams2022/bridgestone_data/data'\n",
    "name_list = os.listdir(data_path)\n",
    "sales_name_list = [x for x in name_list if x[0:7]=='sales_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/data/p_dsi/teams2022/team_1/new_data\"):\n",
    "    os.mkdir(\"/data/p_dsi/teams2022/team_1/new_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(sales_name_list):\n",
    "    new_list = []\n",
    "    for name in sales_name_list:\n",
    "        # read data files and clean names\n",
    "        sale = pd.read_csv(data_path + \"/\" + name, sep='|', skiprows=[1]).clean_names()\n",
    "        individual = pd.read_csv(data_path + '/individual.csv', sep=',', skiprows=[1]).clean_names()\n",
    "        product = pd.read_csv(data_path + '/product.csv', sep='|', skiprows=[1]).clean_names()\n",
    "        store = pd.read_csv(data_path + '/store.csv', sep='|', skiprows=[1]).clean_names()\n",
    "        vehicle = pd.read_csv(data_path + '/vehicle.csv', sep='|', skiprows=[1]).clean_names()\n",
    "        \n",
    "        # convert store zip and id to string\n",
    "        store['store_id'] = store['store_id'].apply(str)\n",
    "        sale['store_id'] = sale['store_id'].apply(str)\n",
    "        store['zip_code'] = store['zip_code'].apply(str)\n",
    "        \n",
    "        # change the column name of table 'individual'\n",
    "        individual = individual.rename(columns={'mzb_indiv_id':'indiv_id'})\n",
    "        \n",
    "        # merging the data sets together\n",
    "        mega_table = sale.merge(product, on = 'article_id', how = 'left').\\\n",
    "            merge(store, on = 'store_id', how = 'left').\\\n",
    "            merge(individual, on = 'indiv_id', how = 'left').\\\n",
    "            merge(vehicle, on = 'vehicle_id', how = 'left')\n",
    "        \n",
    "        # extracting name for storing data sets\n",
    "        new_name = name[6:]\n",
    "        new_list.append(new_name)\n",
    "        mega_table[\"year\"] = new_name[:4]\n",
    "        mega_table['month'] = new_name[4:-4]\n",
    "        mega_table = mega_table[(mega_table['ah1_res_bus_indc'] == 'R') & (mega_table['supp1_bus_pander'] == 'N') & (mega_table['email_optin_ind'] == 'Y')]\n",
    "        mega_table = mega_table.drop(['ah1_res_bus_indc', 'supp1_bus_pander', 'email_optin_ind'], axis=1)\n",
    "        col_list = list(mega_table.columns)\n",
    "        mega_table.to_csv(\"/data/p_dsi/teams2022/team_1/new_data/\" + new_name)\n",
    "    return new_list, col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(sales_list):\n",
    "    data_list, col_list = join_data(sales_list)\n",
    "    df = pd.DataFrame(columns = col_list)\n",
    "    for data_name in data_list: \n",
    "        if os.path.isfile(\"/data/p_dsi/teams2022/team_1/new_data/\" + data_name + \".csv\"):\n",
    "            df1 = pd.read_csv(\"/data/p_dsi/teams2022/team_1/new_data/\" + data_name + \".csv\")\n",
    "            df = pd.concat([df1, df], axis = 0)\n",
    "            df = df.reset_index(drop = True)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df = combine_data(sales_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.to_csv(\"/data/p_dsi/teams2022/team_1/new_data/total_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high probability for ACCRE to break down during the final combination process. So when you run this notebook, it will be better to use a 4GPU (24 cores) server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
